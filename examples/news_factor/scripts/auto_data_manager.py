#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨æ•°æ®ç®¡ç†ç³»ç»Ÿ
åœ¨ç­–ç•¥è¿è¡Œå‰è‡ªåŠ¨æ£€æµ‹å¹¶ä¸‹è½½ç¼ºå¤±çš„æ•°æ®ï¼Œç”Ÿæˆå› å­
"""

import os
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import yaml
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import subprocess
import glob

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

class AutoDataManager:
    """è‡ªåŠ¨æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨"""
        self.config_path = config_path
        self.config = self._load_config()
        self._setup_logging()
        self._setup_directories()
        
        # æ•°æ®è·¯å¾„
        self.price_data_dir = project_root / "data" / "raw" / "price"
        self.news_data_dir = project_root / "data" / "raw" / "news"
        self.factors_data_dir = project_root / "data" / "factors"
        self.processed_data_dir = project_root / "data" / "processed"
        
        # è„šæœ¬è·¯å¾„
        self.scripts_dir = project_root / "scripts"
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = project_root / self.config_path
        if not config_file.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def _setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            "data/raw/price",
            "data/raw/news", 
            "data/processed",
            "data/factors",
            "data/results",
            "logs"
        ]
        
        for directory in directories:
            (project_root / directory).mkdir(parents=True, exist_ok=True)
    
    def check_data_availability(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ•°æ®å¯ç”¨æ€§"""
        self.logger.info("ğŸ” æ£€æŸ¥æ•°æ®å¯ç”¨æ€§...")
        
        availability = {
            'price_data': self._check_price_data(),
            'news_data': self._check_news_data(),
            'factor_data': self._check_factor_data()
        }
        
        self.logger.info(f"æ•°æ®æ£€æŸ¥ç»“æœ: {availability}")
        return availability
    
    def _check_price_data(self) -> bool:
        """æ£€æŸ¥ä»·æ ¼æ•°æ®æ˜¯å¦å­˜åœ¨"""
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»·æ ¼æ•°æ®æ–‡ä»¶
        price_files = list(self.price_data_dir.glob("*.csv")) + list(self.price_data_dir.glob("*.parquet"))
        
        if not price_files:
            self.logger.info("âŒ æœªæ‰¾åˆ°ä»·æ ¼æ•°æ®æ–‡ä»¶")
            return False
        
        # æ£€æŸ¥æœ€æ–°æ–‡ä»¶çš„æ—¶é—´èŒƒå›´
        latest_file = sorted(price_files, key=lambda x: x.stat().st_mtime)[-1]
        
        try:
            if latest_file.suffix == '.csv':
                df = pd.read_csv(latest_file, index_col=0, parse_dates=True)
            else:
                df = pd.read_parquet(latest_file)
            
            # æ£€æŸ¥æ•°æ®æ—¶é—´èŒƒå›´
            lookback_days = self.config.get('data', {}).get('price', {}).get('lookback_days', 30)
            required_start = datetime.now() - timedelta(days=lookback_days)
            
            if df.index.max() < required_start:
                self.logger.info(f"âš ï¸ ä»·æ ¼æ•°æ®è¿‡æœŸï¼Œæœ€æ–°æ•°æ®: {df.index.max()}")
                return False
            
            self.logger.info(f"âœ… ä»·æ ¼æ•°æ®å¯ç”¨ï¼Œæ—¶é—´èŒƒå›´: {df.index.min()} - {df.index.max()}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä»·æ ¼æ•°æ®æ–‡ä»¶æŸå: {e}")
            return False
    
    def _check_news_data(self) -> bool:
        """æ£€æŸ¥æ–°é—»æ•°æ®æ˜¯å¦å­˜åœ¨"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°é—»æ•°æ®æ–‡ä»¶
        news_files = list(self.news_data_dir.glob("*.csv")) + list(self.news_data_dir.glob("*.parquet"))
        
        if not news_files:
            self.logger.info("âŒ æœªæ‰¾åˆ°æ–°é—»æ•°æ®æ–‡ä»¶")
            return False
        
        # æ£€æŸ¥æœ€æ–°æ–‡ä»¶çš„å†…å®¹
        latest_file = sorted(news_files, key=lambda x: x.stat().st_mtime)[-1]
        
        try:
            if latest_file.suffix == '.csv':
                df = pd.read_csv(latest_file)
            else:
                df = pd.read_parquet(latest_file)
            
            if len(df) == 0:
                self.logger.info("âŒ æ–°é—»æ•°æ®æ–‡ä»¶ä¸ºç©º")
                return False
            
            # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['title', 'description', 'published']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                self.logger.info(f"âš ï¸ æ–°é—»æ•°æ®ç¼ºå°‘åˆ—: {missing_columns}")
                return False
            
            self.logger.info(f"âœ… æ–°é—»æ•°æ®å¯ç”¨ï¼Œå…± {len(df)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ–°é—»æ•°æ®æ–‡ä»¶æŸå: {e}")
            return False
    
    def _check_factor_data(self) -> bool:
        """æ£€æŸ¥å› å­æ•°æ®æ˜¯å¦å­˜åœ¨"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å› å­æ•°æ®æ–‡ä»¶
        factor_files = list(self.factors_data_dir.glob("*.parquet")) + list(self.factors_data_dir.glob("*.csv"))
        
        if not factor_files:
            self.logger.info("âŒ æœªæ‰¾åˆ°å› å­æ•°æ®æ–‡ä»¶")
            return False
        
        # æ£€æŸ¥æœ€æ–°æ–‡ä»¶çš„å†…å®¹
        latest_file = sorted(factor_files, key=lambda x: x.stat().st_mtime)[-1]
        
        try:
            if latest_file.suffix == '.csv':
                df = pd.read_csv(latest_file, index_col=0, parse_dates=True)
            else:
                df = pd.read_parquet(latest_file)
            
            if len(df) == 0:
                self.logger.info("âŒ å› å­æ•°æ®æ–‡ä»¶ä¸ºç©º")
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬çš„å› å­åˆ—
            expected_factors = ['sentiment_score', 'news_volume', 'price_return']
            available_factors = [col for col in expected_factors if any(col in df_col for df_col in df.columns)]
            
            if len(available_factors) == 0:
                self.logger.info("âš ï¸ å› å­æ•°æ®ä¸­æœªæ‰¾åˆ°é¢„æœŸçš„å› å­")
                return False
            
            self.logger.info(f"âœ… å› å­æ•°æ®å¯ç”¨ï¼Œå…± {df.shape[1]} ä¸ªå› å­ï¼Œ{df.shape[0]} ä¸ªæ—¶é—´ç‚¹")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å› å­æ•°æ®æ–‡ä»¶æŸå: {e}")
            return False
    
    def download_price_data(self) -> bool:
        """ä¸‹è½½ä»·æ ¼æ•°æ®ï¼ˆæ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„æ—¶é—´èŒƒå›´ï¼‰"""
        self.logger.info("ğŸ“¥ å¼€å§‹ä¸‹è½½ä»·æ ¼æ•°æ®...")
        
        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æ—¶é—´èŒƒå›´
        start_date = None
        end_date = None
        try:
            backtest_config = self.config.get('evaluation', {}).get('backtest', {})
            start_date = backtest_config.get('start_date')
            end_date = backtest_config.get('end_date')
            
            if start_date:
                self.logger.info(f"ğŸ“… é…ç½®çš„å¼€å§‹æ—¥æœŸ: {start_date}")
            if end_date:
                self.logger.info(f"ğŸ“… é…ç½®çš„ç»“æŸæ—¥æœŸ: {end_date}")
                
        except Exception as e:
            self.logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶é—´èŒƒå›´å¤±è´¥: {e}")
        
        try:
            # æ„å»ºå‘½ä»¤ - ä¸ä¼ é€’dayså‚æ•°ï¼Œè®©data_collection.pyä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ—¶é—´èŒƒå›´
            cmd = [
                sys.executable,
                str(self.scripts_dir / "data_collection.py"),
                "--data-type", "price"
            ]
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=project_root)
            
            if result.returncode == 0:
                self.logger.info("âœ… ä»·æ ¼æ•°æ®ä¸‹è½½å®Œæˆ")
                return True
            else:
                self.logger.error(f"âŒ ä»·æ ¼æ•°æ®ä¸‹è½½å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ä»·æ ¼æ•°æ®ä¸‹è½½å¼‚å¸¸: {e}")
            return False
    
    def download_news_data(self) -> bool:
        """ä¸‹è½½æ–°é—»æ•°æ®ï¼ˆæ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„æ—¶é—´èŒƒå›´ï¼‰"""
        self.logger.info("ğŸ“° å¼€å§‹ä¸‹è½½æ–°é—»æ•°æ®...")
        
        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æ—¶é—´èŒƒå›´
        start_date = None
        end_date = None
        try:
            backtest_config = self.config.get('evaluation', {}).get('backtest', {})
            start_date = backtest_config.get('start_date')
            end_date = backtest_config.get('end_date')
            
            if start_date:
                self.logger.info(f"ğŸ“… é…ç½®çš„å¼€å§‹æ—¥æœŸ: {start_date}")
            if end_date:
                self.logger.info(f"ğŸ“… é…ç½®çš„ç»“æŸæ—¥æœŸ: {end_date}")
                
        except Exception as e:
            self.logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶é—´èŒƒå›´å¤±è´¥: {e}")
        
        try:
            # æ„å»ºå‘½ä»¤
            cmd = [
                sys.executable,
                str(self.scripts_dir / "data_collection.py"),
                "--data-type", "news"
            ]
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=project_root)
            
            if result.returncode == 0:
                self.logger.info("âœ… æ–°é—»æ•°æ®ä¸‹è½½å®Œæˆ")
                return True
            else:
                self.logger.error(f"âŒ æ–°é—»æ•°æ®ä¸‹è½½å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æ–°é—»æ•°æ®ä¸‹è½½å¼‚å¸¸: {e}")
            return False
    
    def generate_factors(self) -> bool:
        """ç”Ÿæˆå› å­æ•°æ®"""
        self.logger.info("ğŸ”§ å¼€å§‹ç”Ÿæˆå› å­æ•°æ®...")
        
        try:
            # æ„å»ºå‘½ä»¤
            cmd = [
                sys.executable,
                str(self.scripts_dir / "factor_construction.py")
            ]
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=project_root)
            
            if result.returncode == 0:
                self.logger.info("âœ… å› å­æ•°æ®ç”Ÿæˆå®Œæˆ")
                return True
            else:
                self.logger.error(f"âŒ å› å­æ•°æ®ç”Ÿæˆå¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ å› å­æ•°æ®ç”Ÿæˆå¼‚å¸¸: {e}")
            return False
    
    def ensure_data_ready(self) -> bool:
        """ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å‡†å¤‡å°±ç»ª"""
        self.logger.info("ğŸš€ å¼€å§‹è‡ªåŠ¨æ•°æ®ç®¡ç†æµç¨‹...")
        
        # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
        availability = self.check_data_availability()
        
        success = True
        
        # å¤„ç†ä»·æ ¼æ•°æ®
        if not availability['price_data']:
            self.logger.info("éœ€è¦ä¸‹è½½ä»·æ ¼æ•°æ®...")
            if not self.download_price_data():
                success = False
        
        # å¤„ç†æ–°é—»æ•°æ®
        if not availability['news_data']:
            self.logger.info("éœ€è¦ä¸‹è½½æ–°é—»æ•°æ®...")
            if not self.download_news_data():
                success = False
        
        # å¤„ç†å› å­æ•°æ®
        if not availability['factor_data']:
            self.logger.info("éœ€è¦ç”Ÿæˆå› å­æ•°æ®...")
            # å¦‚æœä»·æ ¼æ•°æ®æˆ–æ–°é—»æ•°æ®åˆšä¸‹è½½ï¼Œä¹Ÿéœ€è¦é‡æ–°ç”Ÿæˆå› å­
            if not self.generate_factors():
                success = False
        elif not availability['price_data'] or not availability['news_data']:
            # å¦‚æœä»·æ ¼æˆ–æ–°é—»æ•°æ®æ›´æ–°äº†ï¼Œé‡æ–°ç”Ÿæˆå› å­
            self.logger.info("æ•°æ®æ›´æ–°ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆå› å­...")
            if not self.generate_factors():
                success = False
        
        if success:
            self.logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼")
        else:
            self.logger.error("âš ï¸ æ•°æ®å‡†å¤‡è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜")
        
        return success
    
    def get_data_summary(self) -> Dict:
        """è·å–æ•°æ®æ‘˜è¦ä¿¡æ¯"""
        summary = {
            'price_data': {},
            'news_data': {},
            'factor_data': {}
        }
        
        # ä»·æ ¼æ•°æ®æ‘˜è¦
        try:
            price_files = list(self.price_data_dir.glob("*.csv")) + list(self.price_data_dir.glob("*.parquet"))
            if price_files:
                latest_file = sorted(price_files, key=lambda x: x.stat().st_mtime)[-1]
                if latest_file.suffix == '.csv':
                    df = pd.read_csv(latest_file, index_col=0, parse_dates=True)
                else:
                    df = pd.read_parquet(latest_file)
                
                summary['price_data'] = {
                    'file_count': len(price_files),
                    'latest_file': latest_file.name,
                    'record_count': len(df),
                    'date_range': f"{df.index.min()} - {df.index.max()}",
                    'columns': list(df.columns)
                }
        except Exception as e:
            summary['price_data']['error'] = str(e)
        
        # æ–°é—»æ•°æ®æ‘˜è¦
        try:
            news_files = list(self.news_data_dir.glob("*.csv")) + list(self.news_data_dir.glob("*.parquet"))
            if news_files:
                latest_file = sorted(news_files, key=lambda x: x.stat().st_mtime)[-1]
                if latest_file.suffix == '.csv':
                    df = pd.read_csv(latest_file)
                else:
                    df = pd.read_parquet(latest_file)
                
                summary['news_data'] = {
                    'file_count': len(news_files),
                    'latest_file': latest_file.name,
                    'record_count': len(df),
                    'columns': list(df.columns)
                }
        except Exception as e:
            summary['news_data']['error'] = str(e)
        
        # å› å­æ•°æ®æ‘˜è¦
        try:
            factor_files = list(self.factors_data_dir.glob("*.parquet")) + list(self.factors_data_dir.glob("*.csv"))
            if factor_files:
                latest_file = sorted(factor_files, key=lambda x: x.stat().st_mtime)[-1]
                if latest_file.suffix == '.csv':
                    df = pd.read_csv(latest_file, index_col=0, parse_dates=True)
                else:
                    df = pd.read_parquet(latest_file)
                
                summary['factor_data'] = {
                    'file_count': len(factor_files),
                    'latest_file': latest_file.name,
                    'factor_count': df.shape[1],
                    'record_count': df.shape[0],
                    'factors': list(df.columns)
                }
        except Exception as e:
            summary['factor_data']['error'] = str(e)
        
        return summary
    
    def clean_old_data(self, keep_days: int = 7) -> bool:
        """æ¸…ç†æ—§æ•°æ®æ–‡ä»¶"""
        self.logger.info(f"ğŸ§¹ æ¸…ç† {keep_days} å¤©å‰çš„æ—§æ•°æ®...")
        
        cutoff_time = datetime.now() - timedelta(days=keep_days)
        cleaned_count = 0
        
        for data_dir in [self.price_data_dir, self.news_data_dir, self.factors_data_dir]:
            if not data_dir.exists():
                continue
                
            for file_path in data_dir.glob("*"):
                if file_path.is_file():
                    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_time < cutoff_time:
                        try:
                            file_path.unlink()
                            cleaned_count += 1
                            self.logger.info(f"å·²åˆ é™¤æ—§æ–‡ä»¶: {file_path.name}")
                        except Exception as e:
                            self.logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
        
        self.logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ–‡ä»¶")
        return True


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è‡ªåŠ¨æ•°æ®ç®¡ç†å·¥å…·")
    parser.add_argument("--check", action="store_true", help="åªæ£€æŸ¥æ•°æ®å¯ç”¨æ€§")
    parser.add_argument("--download-price", action="store_true", help="å¼ºåˆ¶ä¸‹è½½ä»·æ ¼æ•°æ®")
    parser.add_argument("--download-news", action="store_true", help="å¼ºåˆ¶ä¸‹è½½æ–°é—»æ•°æ®")
    parser.add_argument("--generate-factors", action="store_true", help="å¼ºåˆ¶ç”Ÿæˆå› å­æ•°æ®")
    parser.add_argument("--summary", action="store_true", help="æ˜¾ç¤ºæ•°æ®æ‘˜è¦")
    parser.add_argument("--clean", type=int, metavar="DAYS", help="æ¸…ç†Nå¤©å‰çš„æ—§æ•°æ®")
    parser.add_argument("--config", default="configs/config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    try:
        manager = AutoDataManager(args.config)
        
        if args.check:
            # åªæ£€æŸ¥æ•°æ®å¯ç”¨æ€§
            availability = manager.check_data_availability()
            print(f"\næ•°æ®å¯ç”¨æ€§æ£€æŸ¥ç»“æœ:")
            print(f"ä»·æ ¼æ•°æ®: {'âœ…' if availability['price_data'] else 'âŒ'}")
            print(f"æ–°é—»æ•°æ®: {'âœ…' if availability['news_data'] else 'âŒ'}")
            print(f"å› å­æ•°æ®: {'âœ…' if availability['factor_data'] else 'âŒ'}")
            
        elif args.download_price:
            # å¼ºåˆ¶ä¸‹è½½ä»·æ ¼æ•°æ®
            success = manager.download_price_data()
            return 0 if success else 1
            
        elif args.download_news:
            # å¼ºåˆ¶ä¸‹è½½æ–°é—»æ•°æ®
            success = manager.download_news_data()
            return 0 if success else 1
            
        elif args.generate_factors:
            # å¼ºåˆ¶ç”Ÿæˆå› å­æ•°æ®
            success = manager.generate_factors()
            return 0 if success else 1
            
        elif args.summary:
            # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
            summary = manager.get_data_summary()
            print(f"\nğŸ“Š æ•°æ®æ‘˜è¦æŠ¥å‘Š:")
            print(f"{'='*50}")
            
            for data_type, info in summary.items():
                print(f"\n{data_type.upper()}:")
                if 'error' in info:
                    print(f"  âŒ é”™è¯¯: {info['error']}")
                else:
                    for key, value in info.items():
                        print(f"  {key}: {value}")
            
        elif args.clean:
            # æ¸…ç†æ—§æ•°æ®
            success = manager.clean_old_data(args.clean)
            return 0 if success else 1
            
        else:
            # é»˜è®¤æ‰§è¡Œå®Œæ•´çš„æ•°æ®å‡†å¤‡æµç¨‹
            success = manager.ensure_data_ready()
            return 0 if success else 1
            
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨æ•°æ®ç®¡ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main()) 