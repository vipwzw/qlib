#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»æƒ…æ„Ÿå› å­åˆ†æ - æ•°æ®é‡‡é›†è„šæœ¬
æ”¯æŒBTC/USDTä»·æ ¼æ•°æ®å’Œå¤šæºæ–°é—»æ•°æ®é‡‡é›†
"""

import argparse
import sys
import os
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from typing import Dict, List, Optional
import yaml
import logging
import time
import requests
import feedparser
from urllib.parse import urljoin

# å¯¼å…¥éå°å·APIæŠ“å–å™¨
try:
    from feixiaohao_api_scraper import FeixiaohaoAPIScraper
    HAS_FEIXIAOHAO_SCRAPER = True
except ImportError:
    HAS_FEIXIAOHAO_SCRAPER = False

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# æ·»åŠ utilsè·¯å¾„ä»¥ä½¿ç”¨é…ç½®åŠ è½½å™¨
utils_path = project_root / "utils"
if utils_path.exists():
    sys.path.append(str(utils_path))
    try:
        from config_loader import ConfigLoader
        HAS_CONFIG_LOADER = True
    except ImportError:
        HAS_CONFIG_LOADER = False
else:
    HAS_CONFIG_LOADER = False

class DataCollectionManager:
    """æ•°æ®é‡‡é›†ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–æ•°æ®é‡‡é›†ç®¡ç†å™¨"""
        self.config_path = config_path
        self.config = self._load_config()
        self._setup_logging()
        self._setup_directories()
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if HAS_CONFIG_LOADER:
            try:
                # ä½¿ç”¨é…ç½®åŠ è½½å™¨åŠ è½½ç¯å¢ƒå˜é‡
                loader = ConfigLoader(config_file=self.config_path)
                return loader.load_config()
            except Exception as e:
                self.logger.warning(f"é…ç½®åŠ è½½å™¨å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼: {e}")
        
        # ä¼ ç»Ÿæ–¹å¼åŠ è½½é…ç½®
        config_file = project_root / self.config_path
        if not config_file.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get('logging', {})
        logging.basicConfig(
            level=getattr(logging, log_config.get('level', 'INFO')),
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def _setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            "data/raw/price",
            "data/raw/news", 
            "data/processed",
            "data/factors",
            "logs"
        ]
        
        for directory in directories:
            (project_root / directory).mkdir(parents=True, exist_ok=True)

    def collect_price_data(self, days: int = 30) -> bool:
        """
        ä»Binance APIé‡‡é›†çœŸå®çš„BTC/USDT 1åˆ†é’ŸKçº¿æ•°æ®
        æ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„å›æµ‹æ—¶é—´èŒƒå›´ä¸‹è½½æ•°æ®
        """
        
        # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶è¯»å–æ—¶é—´èŒƒå›´
        start_date = None
        end_date = None
        try:
            backtest_config = self.config.get('evaluation', {}).get('backtest', {})
            start_date = backtest_config.get('start_date')
            end_date = backtest_config.get('end_date')
            
            if start_date and end_date:
                self.logger.info(f"ğŸ“… ä½¿ç”¨é…ç½®æ–‡ä»¶æ—¶é—´èŒƒå›´: {start_date} - {end_date}")
            else:
                self.logger.info(f"âš ï¸ é…ç½®æ–‡ä»¶æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤å›çœ‹ {days} å¤©")
                
        except Exception as e:
            self.logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶é—´èŒƒå›´å¤±è´¥: {e}")
            
        self.logger.info(f"å¼€å§‹ä»Binance APIé‡‡é›†BTC/USDTä»·æ ¼æ•°æ®")
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆBinance APIä½¿ç”¨æ¯«ç§’æ—¶é—´æˆ³ï¼‰
            if start_date and end_date:
                # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ—¶é—´èŒƒå›´
                start_time = datetime.strptime(start_date, '%Y-%m-%d')
                end_time = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1)  # åŒ…å«ç»“æŸæ—¥æœŸ
            else:
                # ä½¿ç”¨é»˜è®¤å¤©æ•°
                end_time = datetime.now()
                start_time = end_time - timedelta(days=days)
            
            start_timestamp = int(start_time.timestamp() * 1000)
            end_timestamp = int(end_time.timestamp() * 1000)
            
            self.logger.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {start_time} - {end_time}")
            
            # Binance APIé…ç½®
            base_url = "https://api.binance.com"
            symbol = "BTCUSDT"
            interval = "1m"  # 1åˆ†é’ŸKçº¿
            limit = 1000  # æ¯æ¬¡è¯·æ±‚æœ€å¤š1000æ¡æ•°æ®
            
            all_data = []
            current_start = start_timestamp
            
            # åˆ†æ‰¹è·å–æ•°æ®ï¼ˆå› ä¸ºAPIæœ‰å•æ¬¡è¯·æ±‚é™åˆ¶ï¼‰
            while current_start < end_timestamp:
                try:
                    # æ„å»ºAPIè¯·æ±‚URL
                    params = {
                        'symbol': symbol,
                        'interval': interval,
                        'startTime': current_start,
                        'endTime': end_timestamp,
                        'limit': limit
                    }
                    
                    url = f"{base_url}/api/v3/klines"
                    
                    self.logger.info(f"è¯·æ±‚æ•°æ®: {datetime.fromtimestamp(current_start/1000)}")
                    
                    # å‘é€è¯·æ±‚
                    response = requests.get(url, params=params, timeout=30)
                    
                    if response.status_code != 200:
                        self.logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                        break
                    
                    data = response.json()
                    
                    if not data:
                        self.logger.info("æ²¡æœ‰æ›´å¤šæ•°æ®")
                        break
                    
                    # æ·»åŠ æ•°æ®åˆ°åˆ—è¡¨
                    all_data.extend(data)
                    
                    # æ›´æ–°ä¸‹ä¸€æ‰¹çš„å¼€å§‹æ—¶é—´ï¼ˆæœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´ + 1åˆ†é’Ÿï¼‰
                    last_timestamp = data[-1][0]
                    current_start = last_timestamp + 60000  # åŠ 1åˆ†é’Ÿï¼ˆ60000æ¯«ç§’ï¼‰
                    
                    self.logger.info(f"è·å–åˆ° {len(data)} æ¡æ•°æ®ï¼Œæ€»è®¡ {len(all_data)} æ¡")
                    
                    # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…è§¦å‘APIé™åˆ¶
                    import time
                    time.sleep(0.1)
                    
                except requests.RequestException as e:
                    self.logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
                    break
                except Exception as e:
                    self.logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
                    break
            
            if not all_data:
                self.logger.error("æœªèƒ½è·å–ä»»ä½•ä»·æ ¼æ•°æ®")
                return False
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            self.logger.info(f"å¼€å§‹å¤„ç† {len(all_data)} æ¡Kçº¿æ•°æ®")
            
            # Binance Kçº¿æ•°æ®æ ¼å¼:
            # [æ—¶é—´æˆ³, å¼€ç›˜ä»·, æœ€é«˜ä»·, æœ€ä½ä»·, æ”¶ç›˜ä»·, æˆäº¤é‡, æ”¶ç›˜æ—¶é—´, æˆäº¤é¢, æˆäº¤ç¬”æ•°, ä¸»åŠ¨ä¹°å…¥æˆäº¤é‡, ä¸»åŠ¨ä¹°å…¥æˆäº¤é¢, å¿½ç•¥]
            processed_data = []
            
            for kline in all_data:
                timestamp = datetime.fromtimestamp(kline[0] / 1000)
                processed_data.append({
                    'timestamp': timestamp,
                    'open': float(kline[1]),
                    'high': float(kline[2]),
                    'low': float(kline[3]),
                    'close': float(kline[4]),
                    'volume': float(kline[5])
                })
            
            # åˆ›å»ºDataFrame
            price_data = pd.DataFrame(processed_data)
            
            # æŒ‰æ—¶é—´æ’åº
            price_data = price_data.sort_values('timestamp').reset_index(drop=True)
            
            # å»é‡ï¼ˆä»¥é˜²APIè¿”å›é‡å¤æ•°æ®ï¼‰
            price_data = price_data.drop_duplicates(subset=['timestamp']).reset_index(drop=True)
            
            # éªŒè¯æ•°æ®è´¨é‡
            self.logger.info(f"æ•°æ®éªŒè¯:")
            self.logger.info(f"  æ—¶é—´èŒƒå›´: {price_data['timestamp'].min()} - {price_data['timestamp'].max()}")
            self.logger.info(f"  ä»·æ ¼èŒƒå›´: ${price_data['close'].min():.2f} - ${price_data['close'].max():.2f}")
            self.logger.info(f"  å¹³å‡æˆäº¤é‡: {price_data['volume'].mean():.2f}")
            
            # ä¿å­˜æ•°æ®
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"btc_usdt_1m_real_{timestamp_str}.csv"
            output_path = project_root / "data" / "raw" / "price" / filename
            
            price_data.to_csv(output_path, index=False)
            
            self.logger.info(f"âœ… çœŸå®ä»·æ ¼æ•°æ®é‡‡é›†å®Œæˆï¼Œå…± {len(price_data)} æ¡è®°å½•")
            self.logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä»·æ ¼æ•°æ®é‡‡é›†å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return False

    def collect_news_data(self) -> bool:
        """é‡‡é›†æ–°é—»æ•°æ®"""
        self.logger.info("å¼€å§‹é‡‡é›†æ–°é—»æ•°æ®")
        
        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æ—¶é—´èŒƒå›´
        start_date = None
        end_date = None
        try:
            backtest_config = self.config.get('evaluation', {}).get('backtest', {})
            start_date = backtest_config.get('start_date')
            end_date = backtest_config.get('end_date')
            
            if start_date:
                self.logger.info(f"ğŸ“… é…ç½®æ–‡ä»¶æŒ‡å®šå¼€å§‹æ—¥æœŸ: {start_date}")
            if end_date:
                self.logger.info(f"ğŸ“… é…ç½®æ–‡ä»¶æŒ‡å®šç»“æŸæ—¥æœŸ: {end_date}")
                
        except Exception as e:
            self.logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶é—´èŒƒå›´å¤±è´¥: {e}")
        
        try:
            news_sources = self.config.get('news_sources', {})
            all_news = []
            
            for source_name, source_config in news_sources.items():
                if not source_config.get('enabled', False):
                    continue
                
                self.logger.info(f"ä» {source_config.get('name', source_name)} é‡‡é›†æ–°é—»...")
                
                if source_config.get('type') == 'rss':
                    news_items = self._collect_rss_news(source_config, start_date, end_date)
                    all_news.extend(news_items)
                    self.logger.info(f"ä» {source_name} é‡‡é›†åˆ° {len(news_items)} æ¡æ–°é—»")
                elif source_config.get('type') == 'api' and source_name == 'feixiaohao':
                    news_items = self._collect_feixiaohao_news(source_config, start_date, end_date)
                    all_news.extend(news_items)
                    self.logger.info(f"ä» {source_name} é‡‡é›†åˆ° {len(news_items)} æ¡æ–°é—»")
            
            if all_news:
                # åˆ›å»ºDataFrame
                news_df = pd.DataFrame(all_news)
                
                # å»é‡å’Œè¿‡æ»¤
                news_df = news_df.drop_duplicates(subset=['title'])
                news_df = self._filter_crypto_news(news_df)
                
                # åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
                if start_date or end_date:
                    original_count = len(news_df)
                    news_df = self._filter_by_date_range(news_df, start_date, end_date)
                    filtered_count = len(news_df)
                    self.logger.info(f"æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼š{original_count} -> {filtered_count} æ¡æ–°é—»")
                
                # ä¿å­˜æ•°æ®
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"crypto_news_{timestamp}.csv"
                output_path = project_root / "data" / "raw" / "news" / filename
                
                news_df.to_csv(output_path, index=False)
                
                self.logger.info(f"è¿‡æ»¤åå‰©ä½™ {len(news_df)} æ¡ç›¸å…³æ–°é—»")
                self.logger.info(f"æ–°é—»æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
                
                return True
            else:
                self.logger.warning("æœªé‡‡é›†åˆ°ä»»ä½•æ–°é—»æ•°æ®")
                return False
                
        except Exception as e:
            self.logger.error(f"æ–°é—»æ•°æ®é‡‡é›†å¤±è´¥: {e}")
            return False

    def _collect_rss_news(self, source_config: Dict, start_date: str = None, end_date: str = None) -> List[Dict]:
        """ä»RSSæºé‡‡é›†æ–°é—»
        
        Args:
            source_config: æ–°é—»æºé…ç½®
            start_date: å¼€å§‹æ—¥æœŸ (é…ç½®æ–‡ä»¶ä¸­è·å–)
            end_date: ç»“æŸæ—¥æœŸ (é…ç½®æ–‡ä»¶ä¸­è·å–)
        """
        news_items = []
        
        try:
            url = source_config.get('url')
            if not url:
                self.logger.warning(f"RSSæºé…ç½®ç¼ºå°‘URL: {source_config.get('name')}")
                return news_items
            
            self.logger.info(f"æ­£åœ¨è®¿é—®RSSæº: {url}")
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                'Accept-Language': 'en-US,en;q=0.9',
                'Cache-Control': 'no-cache'
            }
            
            # ä½¿ç”¨requestsè·å–RSSå†…å®¹
            response = requests.get(url, headers=headers, timeout=15)
            self.logger.info(f"HTTPçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                self.logger.warning(f"HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
                return news_items
            
            # ä½¿ç”¨feedparserè§£æå“åº”å†…å®¹
            feed = feedparser.parse(response.text)
            
            # æ£€æŸ¥è§£æç»“æœ
            if hasattr(feed, 'bozo') and feed.bozo:
                self.logger.warning(f"RSSè§£æå¯èƒ½æœ‰é—®é¢˜: {getattr(feed, 'bozo_exception', 'Unknown error')}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¡ç›®
            if not hasattr(feed, 'entries') or not feed.entries:
                self.logger.warning(f"RSSæºæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¡ç›®: {url}")
                return news_items
            
            self.logger.info(f"RSSæºè¿”å› {len(feed.entries)} ä¸ªæ¡ç›®")
            
            for entry in feed.entries[:20]:  # é™åˆ¶ä¸ºæœ€æ–°20æ¡
                try:
                    # å¤„ç†å‘å¸ƒæ—¶é—´
                    published = entry.get('published', '')
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            published = datetime(*entry.published_parsed[:6]).isoformat()
                        except:
                            published = entry.get('published', '')
                    
                    # æå–æè¿°æˆ–æ‘˜è¦
                    description = entry.get('description', '')
                    if not description:
                        description = entry.get('summary', '')
                    
                    news_item = {
                        'title': entry.get('title', '').strip(),
                        'description': self._clean_text(description),
                        'link': entry.get('link', ''),
                        'published': published,
                        'source': source_config.get('name', 'Unknown')
                    }
                    
                    # åªæ·»åŠ æœ‰æ ‡é¢˜çš„æ–°é—»
                    if news_item['title']:
                        news_items.append(news_item)
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†RSSæ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue
                
        except requests.RequestException as e:
            self.logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ {source_config.get('url')}: {e}")
        except Exception as e:
            self.logger.error(f"RSSé‡‡é›†å¤±è´¥ {source_config.get('url')}: {e}")
            import traceback
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        return news_items
    
    def _collect_feixiaohao_news(self, source_config: Dict, start_date: str = None, end_date: str = None) -> List[Dict]:
        """ä»éå°å·APIé‡‡é›†æ–°é—»
        
        Args:
            source_config: æ–°é—»æºé…ç½®  
            start_date: å¼€å§‹æ—¥æœŸ (é…ç½®æ–‡ä»¶ä¸­è·å–)
            end_date: ç»“æŸæ—¥æœŸ (é…ç½®æ–‡ä»¶ä¸­è·å–)
        """
        news_items = []
        
        try:
            if not HAS_FEIXIAOHAO_SCRAPER:
                self.logger.error("éå°å·APIæŠ“å–å™¨æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿feixiaohao_api_scraper.pyæ–‡ä»¶å­˜åœ¨")
                return news_items
            
            # ä»é…ç½®ä¸­è·å–å‚æ•°
            max_pages = source_config.get('max_pages', 50)
            per_page = source_config.get('per_page', 100)
            
            self.logger.info(f"å¼€å§‹ä»éå°å·APIé‡‡é›†æ–°é—»ï¼Œæœ€å¤š{max_pages}é¡µï¼Œæ¯é¡µ{per_page}æ¡")
            
            # åˆå§‹åŒ–éå°å·æŠ“å–å™¨
            output_dir = project_root / "data" / "raw" / "news"
            scraper = FeixiaohaoAPIScraper(str(output_dir))
            
            # æŠ“å–æ–°é—»æ•°æ®ï¼Œä¼ é€’æ—¶é—´èŒƒå›´å‚æ•°
            news_data = scraper.scrape_news(max_pages=max_pages, per_page=per_page, start_date=start_date, end_date=end_date)
            
            if news_data:
                # è½¬æ¢æ•°æ®æ ¼å¼ä»¥åŒ¹é…å…¶ä»–æ–°é—»æºçš„æ ¼å¼
                for item in news_data:
                    news_item = {
                        'title': item.get('title', ''),
                        'description': item.get('description', ''),
                        'link': item.get('link', ''),
                        'published': item.get('published', ''),
                        'source': item.get('source', 'éå°å·')
                    }
                    
                    # åªæ·»åŠ æœ‰æ ‡é¢˜çš„æ–°é—»
                    if news_item['title']:
                        news_items.append(news_item)
                        
                self.logger.info(f"æˆåŠŸä»éå°å·APIè·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            else:
                self.logger.warning("éå°å·APIæœªè¿”å›ä»»ä½•æ–°é—»æ•°æ®")
                
        except Exception as e:
            self.logger.error(f"éå°å·APIé‡‡é›†å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        return news_items
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        import re
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _filter_by_date_range(self, news_df: pd.DataFrame, start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """æ ¹æ®æ—¶é—´èŒƒå›´è¿‡æ»¤æ–°é—»
        
        Args:
            news_df: æ–°é—»æ•°æ®æ¡†
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»æ•°æ®æ¡†
        """
        if news_df.empty:
            return news_df
        
        try:
            # å¤„ç†å‘å¸ƒæ—¶é—´åˆ—
            if 'published' not in news_df.columns:
                self.logger.warning("æ–°é—»æ•°æ®ä¸­æ²¡æœ‰ 'published' åˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´è¿‡æ»¤")
                return news_df
            
            # è½¬æ¢å‘å¸ƒæ—¶é—´ä¸ºdatetime
            news_df['published_dt'] = pd.to_datetime(news_df['published'], errors='coerce')
            
            # ç§»é™¤æ— æ³•è§£ææ—¶é—´çš„è®°å½•
            before_count = len(news_df)
            news_df = news_df.dropna(subset=['published_dt'])
            after_count = len(news_df)
            
            if before_count != after_count:
                self.logger.info(f"ç§»é™¤äº† {before_count - after_count} æ¡æ— æ³•è§£ææ—¶é—´çš„æ–°é—»")
            
            # åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
            if start_date:
                start_dt = pd.to_datetime(start_date)
                news_df = news_df[news_df['published_dt'] >= start_dt]
                self.logger.info(f"è¿‡æ»¤å¼€å§‹æ—¥æœŸ {start_date} ä¹‹å‰çš„æ–°é—»")
            
            if end_date:
                end_dt = pd.to_datetime(end_date) + pd.Timedelta(days=1)  # åŒ…å«ç»“æŸæ—¥æœŸå½“å¤©
                news_df = news_df[news_df['published_dt'] < end_dt]
                self.logger.info(f"è¿‡æ»¤ç»“æŸæ—¥æœŸ {end_date} ä¹‹åçš„æ–°é—»")
            
            # åˆ é™¤ä¸´æ—¶åˆ—
            news_df = news_df.drop('published_dt', axis=1)
            
            return news_df
            
        except Exception as e:
            self.logger.error(f"æ—¶é—´èŒƒå›´è¿‡æ»¤å¤±è´¥: {e}")
            return news_df

    def _filter_crypto_news(self, news_df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤ä¸åŠ å¯†è´§å¸ç›¸å…³çš„æ–°é—»"""
        # è‹±æ–‡å…³é”®è¯
        crypto_keywords_en = [
            'bitcoin', 'btc', 'cryptocurrency', 'crypto', 'blockchain',
            'ethereum', 'trading', 'market', 'price', 'investment', 'eth',
            'usdt', 'tether', 'binance', 'coinbase', 'defi', 'nft',
            # ç¨³å®šå¸ç›¸å…³
            'stablecoin', 'stable coin', 'usdc', 'dai', 'busd',
            # æŒ–çŸ¿ç›¸å…³å…¬å¸
            'core scientific', 'coreweave', 'marathon', 'riot', 'bitfarms',
            'hut 8', 'cleanspark', 'bitdeer', 'canaan', 'bitmain',
            # æŒ–çŸ¿ç›¸å…³æœ¯è¯­
            'mining', 'miner', 'hashrate', 'hash rate', 'asic', 'proof of work',
            # å…¶ä»–é‡è¦æœ¯è¯­
            'wallet', 'exchange', 'custody', 'hodl', 'satoshi', 'wei',
            'smart contract', 'dapp', 'layer 2', 'lightning network',
            # é‡è¦å…¬å¸å’Œé¡¹ç›®
            'microstrategy', 'tesla', 'grayscale', 'blackrock', 'fidelity',
            'solana', 'cardano', 'polkadot', 'chainlink', 'uniswap'
        ]
        
        # ä¸­æ–‡å…³é”®è¯ï¼ˆé€‚ç”¨äºéå°å·ç­‰ä¸­æ–‡æ–°é—»æºï¼‰
        crypto_keywords_cn = [
            'æ¯”ç‰¹å¸', 'BTC', 'ä»¥å¤ªåŠ', 'ETH', 'åŠ å¯†è´§å¸', 'æ•°å­—è´§å¸', 'è™šæ‹Ÿè´§å¸',
            'åŒºå—é“¾', 'å¸å®‰', 'äº¤æ˜“æ‰€', 'æŒ–çŸ¿', 'é’±åŒ…', 'USDT', 'æ³°è¾¾å¸',
            'å»ä¸­å¿ƒåŒ–', 'DeFi', 'NFT', 'ä»£å¸', 'åˆçº¦', 'å…¬é“¾', 'ç§é“¾',
            'çŸ¿æœº', 'çŸ¿æ± ', 'åˆ†å‰', 'ç¡¬åˆ†å‰', 'è½¯åˆ†å‰', 'ç¨³å®šå¸',
            'æŒ–çŸ¿', 'çŸ¿å·¥', 'ç®—åŠ›', 'å“ˆå¸Œç‡', 'å‡åŠ', 'é“¾ä¸Š', 'é“¾ä¸‹',
            'é’±åŒ…åœ°å€', 'ç§é’¥', 'åŠ©è®°è¯', 'å†·é’±åŒ…', 'çƒ­é’±åŒ…',
            # é‡è¦å…¬å¸å’Œé¡¹ç›®åç§°
            'å¾®ç­–ç•¥', 'MicroStrategy', 'ç°åº¦', 'Grayscale', 'è´è±å¾·', 'BlackRock',
            'å˜‰æ¥ ç§‘æŠ€', 'æ¯”ç‰¹å¤§é™†', 'Bitmain', 'èš‚èšçŸ¿æœº', 'Antminer',
            'å¸å®‰ç½‘', 'Binance', 'æ¬§æ˜“', 'OKX', 'ç«å¸', 'Huobi',
            # å…¶ä»–æœ¯è¯­
            'æ™ºèƒ½åˆçº¦', 'å»ä¸­å¿ƒåŒ–åº”ç”¨', 'DApp', 'å…ƒå®‡å®™', 'é—ªç”µç½‘ç»œ',
            'åˆ†å¸ƒå¼è´¦æœ¬', 'å…±è¯†æœºåˆ¶', 'å·¥ä½œé‡è¯æ˜', 'æƒç›Šè¯æ˜',
            'Layer2', 'äºŒå±‚ç½‘ç»œ', 'è·¨é“¾', 'ä¾§é“¾', 'åŸå­äº¤æ¢',
            'æµåŠ¨æ€§', 'åšå¸‚å•†', 'å¥—åˆ©', 'é‡åŒ–äº¤æ˜“', 'é«˜é¢‘äº¤æ˜“'
        ]
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        all_keywords = crypto_keywords_en + crypto_keywords_cn
        
        # åˆ›å»ºå…³é”®è¯è¿‡æ»¤æ¡ä»¶ï¼ˆå¯¹äºä¸­æ–‡å…³é”®è¯ï¼Œä¸è½¬æ¢å¤§å°å†™ï¼‰
        # å…ˆæ£€æŸ¥è‹±æ–‡å…³é”®è¯ï¼ˆè½¬å°å†™ï¼‰
        title_filter_en = news_df['title'].str.lower().str.contains('|'.join(crypto_keywords_en), na=False)
        desc_filter_en = news_df['description'].str.lower().str.contains('|'.join(crypto_keywords_en), na=False)
        
        # å†æ£€æŸ¥ä¸­æ–‡å…³é”®è¯ï¼ˆä¸è½¬å¤§å°å†™ï¼‰
        title_filter_cn = news_df['title'].str.contains('|'.join(crypto_keywords_cn), na=False)
        desc_filter_cn = news_df['description'].str.contains('|'.join(crypto_keywords_cn), na=False)
        
        # è¿”å›åŒ…å«ä»»ä½•å…³é”®è¯çš„æ–°é—»
        return news_df[(title_filter_en | desc_filter_en) | (title_filter_cn | desc_filter_cn)]

    def collect_data(self, data_type: str, days: int = 30) -> bool:
        """ç»Ÿä¸€æ•°æ®é‡‡é›†æ¥å£"""
        success = True
        
        if data_type in ['price', 'all']:
            if not self.collect_price_data(days):
                success = False
        
        if data_type in ['news', 'all']:
            if not self.collect_news_data():
                success = False
        
        return success


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ–°é—»æƒ…æ„Ÿå› å­æ•°æ®é‡‡é›†å·¥å…·")
    parser.add_argument("--data-type", choices=["price", "news", "all"], 
                       default="all", help="é‡‡é›†çš„æ•°æ®ç±»å‹")
    parser.add_argument("--days", type=int, default=30, 
                       help="ä»·æ ¼æ•°æ®å›çœ‹å¤©æ•°")
    parser.add_argument("--config", default="configs/config.yaml", 
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ•°æ®é‡‡é›†ç®¡ç†å™¨
    try:
        manager = DataCollectionManager(args.config)
        
        # æ‰§è¡Œæ•°æ®é‡‡é›†
        success = manager.collect_data(args.data_type, args.days)
        
        if success:
            print("âœ… æ•°æ®é‡‡é›†ä»»åŠ¡å®Œæˆï¼")
        else:
            print("âš ï¸ æ•°æ®é‡‡é›†å®Œæˆï¼Œä½†éƒ¨åˆ†ä»»åŠ¡å¤±è´¥")
            return 1
            
    except Exception as e:
        print(f"âŒ æ•°æ®é‡‡é›†å¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 