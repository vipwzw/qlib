#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éå°å·APIæ–°é—»æŠ“å–å™¨
ä½¿ç”¨æ­£ç¡®çš„åˆ†é¡µé€»è¾‘ï¼šæ—¶é—´æˆ³é€’å‡å‘å†å²æ–¹å‘ç¿»é¡µ
"""

import time
import requests
import pandas as pd
import json
import brotli
from datetime import datetime, timedelta
from pathlib import Path
import logging
from typing import List, Dict, Optional
import argparse


class FeixiaohaoAPIScraper:
    """éå°å·APIæ–°é—»æŠ“å–å™¨"""
    
    def __init__(self, output_dir: str = "data/raw/news"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.api_url = "https://api.fxhnews.com/api/v4/news/news"
        self._setup_logging()
        self._setup_session()
        
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
    def _setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯"""
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'application/json, text/plain, */*',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'zh-CN,zh-Hans;q=0.9',
            'Origin': 'https://feixiaohao.com.cn',
            'Referer': 'https://feixiaohao.com.cn/',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15'
        })
        
    def _decode_response(self, response) -> Dict:
        """è§£ç Brotliå‹ç¼©çš„å“åº”"""
        try:
            return response.json()
        except json.JSONDecodeError:
            # APIä½¿ç”¨Brotliå‹ç¼©
            try:
                decompressed = brotli.decompress(response.content)
                return json.loads(decompressed.decode('utf-8'))
            except Exception as e:
                self.logger.error(f"è§£ç å“åº”å¤±è´¥: {e}")
                return {}
    
    def scrape_news(self, max_pages: int = 50, per_page: int = 100, start_date: str = None, end_date: str = None) -> List[Dict]:
        """æŠ“å–æ–°é—»æ•°æ®"""
        if start_date and end_date:
            self.logger.info(f"å¼€å§‹æŠ“å–æ–°é—»æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date} - {end_date}ï¼Œæœ€å¤š{max_pages}é¡µï¼Œæ¯é¡µ{per_page}æ¡...")
        else:
            self.logger.info(f"å¼€å§‹æŠ“å–æ–°é—»æ•°æ®ï¼Œæœ€å¤š{max_pages}é¡µï¼Œæ¯é¡µ{per_page}æ¡...")
        
        all_news = []
        current_timestamp = None
        
        # è½¬æ¢æ—¶é—´èŒƒå›´ä¸ºæ—¶é—´æˆ³
        start_timestamp = None
        end_timestamp = None
        if start_date:
            try:
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                start_timestamp = int(start_dt.timestamp())
            except ValueError:
                self.logger.warning(f"å¼€å§‹æ—¥æœŸæ ¼å¼é”™è¯¯: {start_date}")
        
        if end_date:
            try:
                end_dt = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1)  # åŒ…å«ç»“æŸæ—¥æœŸ
                end_timestamp = int(end_dt.timestamp())
            except ValueError:
                self.logger.warning(f"ç»“æŸæ—¥æœŸæ ¼å¼é”™è¯¯: {end_date}")
        
        self.logger.info(f"æ—¶é—´æˆ³èŒƒå›´: {start_timestamp} - {end_timestamp}")
        
        # å¦‚æœæä¾›äº†end_dateï¼Œä»end_timestampå¼€å§‹æŠ“å–
        if end_timestamp:
            current_timestamp = end_timestamp
            self.logger.info(f"ä»ç»“æŸæ—¶é—´å¼€å§‹æŠ“å–: {end_date} (timestamp: {end_timestamp})")
        
        base_params = {
            'channelid': 24,
            'direction': 1,
            'per_page': per_page,
            'isfxh': 0,
            'webp': 0
        }
        
        for page in range(max_pages):
            try:
                params = base_params.copy()
                
                if page == 0 and current_timestamp is None:
                    self.logger.info(f"ç¬¬{page+1}é¡µ: é¦–é¡µ")
                else:
                    if current_timestamp is None:
                        self.logger.warning("æ— æ³•è·å–æ—¶é—´æˆ³ï¼Œåœæ­¢æŠ“å–")
                        break
                    params['timestamp'] = current_timestamp
                    self.logger.info(f"ç¬¬{page+1}é¡µ: timestamp={current_timestamp}")
                
                response = self.session.get(self.api_url, params=params, timeout=15)
                
                if response.status_code != 200:
                    self.logger.error(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                    break
                
                data = self._decode_response(response)
                news_items = self._parse_news_list(data)
                
                if not news_items:
                    self.logger.info("æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢æŠ“å–")
                    break
                
                # æ—¶é—´èŒƒå›´è¿‡æ»¤
                filtered_news = []
                should_stop = False
                
                for news in news_items:
                    # è§£ææ–°é—»æ—¶é—´æˆ³
                    published_str = news.get('published', '')
                    try:
                        published_dt = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
                        news_timestamp = int(published_dt.timestamp())
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æŠ“å–åˆ°å¼€å§‹æ—¶é—´ä¹‹å‰çš„æ•°æ®
                        if start_timestamp and news_timestamp < start_timestamp:
                            self.logger.info(f"æ–°é—»æ—¶é—´ {published_str} æ—©äºå¼€å§‹æ—¶é—´ {start_date}ï¼Œåœæ­¢æŠ“å–")
                            should_stop = True
                            break
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                        if end_timestamp and news_timestamp > end_timestamp:
                            # æ–°é—»å¤ªæ–°ï¼Œè·³è¿‡
                            continue
                        
                        # ç¬¦åˆæ—¶é—´èŒƒå›´çš„æ–°é—»
                        filtered_news.append(news)
                        
                    except (ValueError, TypeError) as e:
                        self.logger.warning(f"è§£ææ–°é—»æ—¶é—´å¤±è´¥: {published_str}, {e}")
                        # å¦‚æœæ²¡æœ‰æ—¶é—´é™åˆ¶ï¼Œä»ç„¶ä¿ç•™è¿™æ¡æ–°é—»
                        if not start_timestamp and not end_timestamp:
                            filtered_news.append(news)
                
                # æ·»åŠ ç¬¦åˆæ¡ä»¶çš„æ–°é—»
                if filtered_news:
                    all_news.extend(filtered_news)
                    self.logger.info(f"ç¬¬{page+1}é¡µè·å–åˆ° {len(news_items)} æ¡æ–°é—»ï¼Œè¿‡æ»¤å {len(filtered_news)} æ¡")
                else:
                    self.logger.info(f"ç¬¬{page+1}é¡µè·å–åˆ° {len(news_items)} æ¡æ–°é—»ï¼Œæ— ç¬¦åˆæ—¶é—´èŒƒå›´çš„æ–°é—»")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢æŠ“å–
                if should_stop:
                    self.logger.info(f"å·²æŠ“å–åˆ°ç›®æ ‡æ—¶é—´èŒƒå›´ä¹‹å‰çš„æ•°æ®ï¼Œåœæ­¢æŠ“å–")
                    break
                
                # è·å–ä¸‹ä¸€é¡µæ—¶é—´æˆ³
                current_timestamp = self._get_next_timestamp(data)
                
                if current_timestamp is None:
                    self.logger.warning("æ— æ³•è·å–ä¸‹ä¸€é¡µæ—¶é—´æˆ³ï¼Œåœæ­¢æŠ“å–")
                    break
                
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                self.logger.error(f"ç¬¬{page+1}é¡µæŠ“å–å¤±è´¥: {e}")
                break
        
        self.logger.info(f"æ€»å…±æŠ“å–åˆ° {len(all_news)} æ¡æ–°é—»")
        return all_news
    
    def _parse_news_list(self, data: Dict) -> List[Dict]:
        """è§£ææ–°é—»åˆ—è¡¨"""
        try:
            # æ£€æŸ¥æ•°æ®ç»“æ„
            if not data or 'data' not in data:
                self.logger.warning("å“åº”æ•°æ®ç»“æ„ä¸æ­£ç¡®")
                return []
            
            news_list = data['data'].get('list', [])
            news_items = []
            
            for item in news_list:
                news_item = self._parse_news_item(item)
                if news_item:
                    news_items.append(news_item)
            
            return news_items
            
        except (KeyError, TypeError) as e:
            self.logger.error(f"è§£ææ–°é—»åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def _parse_news_item(self, item: Dict) -> Optional[Dict]:
        """è§£æå•æ¡æ–°é—»"""
        try:
            # æå–æ ‡é¢˜
            title = item.get('title', '').strip()
            if not title or len(title) < 5:
                return None
            
            # æå–å†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œå¦åˆ™ä½¿ç”¨æ ‡é¢˜ï¼‰
            content = item.get('content', title).strip()
            
            # è§£ææ—¶é—´æˆ³
            timestamp = item.get('issuetime')
            if timestamp:
                try:
                    published = datetime.fromtimestamp(int(timestamp)).isoformat()
                except (ValueError, TypeError):
                    published = datetime.now().isoformat()
            else:
                published = datetime.now().isoformat()
            
            # æå–å…¶ä»–å­—æ®µ
            url = item.get('url', '')
            news_id = item.get('id', '')
            
            return {
                'title': title,
                'description': content,
                'link': url,
                'published': published,
                'source': 'éå°å·',
                'source_id': f'feixiaohao_api_{news_id}' if news_id else 'feixiaohao_api'
            }
            
        except Exception as e:
            self.logger.warning(f"è§£ææ–°é—»é¡¹ç›®å¤±è´¥: {e}")
            return None
    
    def _get_next_timestamp(self, data: Dict) -> Optional[int]:
        """è·å–ä¸‹ä¸€é¡µæ—¶é—´æˆ³ï¼ˆå½“å‰é¡µæœ€å°æ—¶é—´æˆ³-1ï¼‰"""
        try:
            if not data or 'data' not in data:
                return None
                
            news_list = data['data'].get('list', [])
            timestamps = []
            
            for item in news_list:
                ts = item.get('issuetime')
                if ts and isinstance(ts, (int, float)):
                    timestamps.append(int(ts))
            
            if timestamps:
                min_timestamp = min(timestamps)
                next_timestamp = min_timestamp - 1  # å‘å†å²æ–¹å‘ç¿»é¡µ
                self.logger.debug(f"æ—¶é—´æˆ³èŒƒå›´: {min(timestamps)} - {max(timestamps)}, ä¸‹ä¸€é¡µ: {next_timestamp}")
                return next_timestamp
                
        except Exception as e:
            self.logger.warning(f"æå–æ—¶é—´æˆ³å¤±è´¥: {e}")
        
        return None
    
    def save_data(self, news_data: List[Dict]) -> str:
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not news_data:
            raise ValueError("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        
        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
        seen_titles = set()
        unique_news = []
        
        for news in news_data:
            title = news.get('title', '').strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(unique_news)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"feixiaohao_news_{timestamp}.csv"
        file_path = self.output_dir / filename
        
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        self.logger.info(f"æ•°æ®å·²ä¿å­˜è‡³: {file_path}")
        self.logger.info(f"å»é‡åå…± {len(unique_news)} æ¡æ–°é—»")
        
        return str(file_path)
    
    def run(self, max_pages: int = 50, per_page: int = 100, start_date: str = None, end_date: str = None) -> str:
        """è¿è¡Œå®Œæ•´çš„æŠ“å–æµç¨‹"""
        try:
            # æŠ“å–æ–°é—»
            news_data = self.scrape_news(max_pages=max_pages, per_page=per_page, start_date=start_date, end_date=end_date)
            
            if not news_data:
                if start_date and end_date:
                    raise ValueError(f"åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´ {start_date} - {end_date} å†…æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
                else:
                    raise ValueError("æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            
            # ä¿å­˜æ•°æ®
            file_path = self.save_data(news_data)
            
            return file_path
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æµç¨‹å¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="éå°å·APIæ–°é—»æŠ“å–å™¨")
    parser.add_argument("--max-pages", type=int, default=200, help="æœ€å¤§æŠ“å–é¡µæ•°")
    parser.add_argument("--per-page", type=int, default=100, help="æ¯é¡µæ–°é—»æ•°é‡") 
    parser.add_argument("--output-dir", default="data/raw/news", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--start-date", type=str, help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    
    args = parser.parse_args()
    
    try:
        scraper = FeixiaohaoAPIScraper(args.output_dir)
        file_path = scraper.run(
            max_pages=args.max_pages, 
            per_page=args.per_page,
            start_date=args.start_date,
            end_date=args.end_date
        )
        
        print(f"âœ… æŠ“å–å®Œæˆï¼æ•°æ®ä¿å­˜è‡³: {file_path}")
        
        if args.start_date and args.end_date:
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {args.start_date} - {args.end_date}")
        
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code) 